//go:build !ignore_autogenerated

// Copyright (c) KAITO authors.
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package v1beta1

import (
	"context"
	"strconv"
	"strings"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"

	"github.com/kaito-project/kaito/pkg/utils/plugin"
)

const (
	// NVIDIA GPU labels for extracting GPU information
	NvidiaGPUCountLabel   = "nvidia.com/gpu.count"
	NvidiaGPUMemoryLabel  = "nvidia.com/gpu.memory"
	NvidiaGPUProductLabel = "nvidia.com/gpu.product"
)

// GPUNodeGroup represents a group of nodes with identical GPU features
// This type is used internally for validation and is not part of the CRD API
// +kubebuilder:object:generate=false
type GPUNodeGroup struct {
	// GPUProduct is the GPU model/product name.
	GPUProduct string
	// GPUCount is the number of GPUs per node.
	GPUCount int64
	// GPUMemory is the memory per GPU.
	GPUMemory resource.Quantity
	// Nodes is the list of node pointers in this bucket
	Nodes []*corev1.Node
}

func CreateGPUNodeGroups(nodeList []corev1.Node) []GPUNodeGroup {
	nodeGroupMap := make(map[string]*GPUNodeGroup)

	for i := range nodeList {
		node := &nodeList[i]

		// Extract GPU information from node labels
		gpuProduct, hasProduct := node.Labels[NvidiaGPUProductLabel]
		gpuCountStr, hasCount := node.Labels[NvidiaGPUCountLabel]
		gpuMemoryStr, hasMemory := node.Labels[NvidiaGPUMemoryLabel]

		// Skip nodes that don't have all required NVIDIA labels
		if !hasProduct || !hasCount || !hasMemory {
			continue
		}

		// Parse GPU count
		gpuCount, err := strconv.ParseInt(gpuCountStr, 10, 64)
		if err != nil {
			continue
		}

		// Parse GPU memory (assume MiB as per NVIDIA GFD documentation)
		var gpuMemory resource.Quantity
		if strings.HasSuffix(gpuMemoryStr, "MiB") {
			memValue, parseErr := strconv.ParseInt(strings.TrimSuffix(gpuMemoryStr, "MiB"), 10, 64)
			if parseErr != nil {
				continue
			}
			gpuMemory = *resource.NewQuantity(memValue*1024*1024, resource.BinarySI) // Convert MiB to bytes
		} else {
			// Try to parse as a standard resource quantity
			var parseErr error
			gpuMemory, parseErr = resource.ParseQuantity(gpuMemoryStr)
			if parseErr != nil {
				continue
			}
		}

		// Create a unique key for this node group configuration
		groupKey := gpuProduct + "_" + gpuCountStr + "_" + gpuMemory.String()

		// Add node to the appropriate group
		if group, exists := nodeGroupMap[groupKey]; exists {
			group.Nodes = append(group.Nodes, node)
		} else {
			nodeGroupMap[groupKey] = &GPUNodeGroup{
				GPUProduct: gpuProduct,
				GPUCount:   gpuCount,
				GPUMemory:  gpuMemory,
				Nodes:      []*corev1.Node{node},
			}
		}
	}

	// Convert map to slice
	result := make([]GPUNodeGroup, 0, len(nodeGroupMap))
	for _, group := range nodeGroupMap {
		result = append(result, *group)
	}

	return result
}

// Filter is used to filter node groups based on custom criteria, such as matching node labels
// This interface is used internally for validation and is not part of the CRD API
// +kubebuilder:object:generate=false
type Filter interface {
	Filter(ctx context.Context, workspace *Workspace, nodeGroups []GPUNodeGroup) []GPUNodeGroup
}

// +kubebuilder:object:generate=false
type GPUCountFilter struct{}

func (f *GPUCountFilter) Filter(ctx context.Context, workspace *Workspace, nodeGroups []GPUNodeGroup) []GPUNodeGroup {
	if workspace.Inference == nil || workspace.Inference.Preset == nil {
		return nodeGroups
	}

	presetName := strings.ToLower(string(workspace.Inference.Preset.Name))
	if !plugin.IsValidPreset(presetName) {
		return []GPUNodeGroup{} // Invalid preset, filter out all
	}

	modelPreset := plugin.KaitoModelRegister.MustGet(presetName)
	params := modelPreset.GetInferenceParameters()
	modelGPUCountRequired := resource.MustParse(params.GPUCountRequirement)
	requiredGPUs := modelGPUCountRequired.Value()

	var filtered []GPUNodeGroup
	for _, group := range nodeGroups {
		// Check if this node group has enough GPUs
		if group.GPUCount >= requiredGPUs {
			filtered = append(filtered, group)
		}
	}

	return filtered
}

// +kubebuilder:object:generate=false
type GPUMemoryFilter struct{}

func (f *GPUMemoryFilter) Filter(ctx context.Context, workspace *Workspace, nodeGroups []GPUNodeGroup) []GPUNodeGroup {
	if workspace.Inference == nil || workspace.Inference.Preset == nil {
		return nodeGroups
	}

	presetName := strings.ToLower(string(workspace.Inference.Preset.Name))
	if !plugin.IsValidPreset(presetName) {
		return []GPUNodeGroup{} // Invalid preset, filter out all
	}

	modelPreset := plugin.KaitoModelRegister.MustGet(presetName)
	params := modelPreset.GetInferenceParameters()
	modelTotalGPUMemoryRequired := resource.MustParse(params.TotalSafeTensorFileSize)

	var filtered []GPUNodeGroup
	for _, group := range nodeGroups {
		// Calculate total GPU memory available per node
		totalGPUMemoryPerNode := resource.NewQuantity(group.GPUMemory.Value()*group.GPUCount, resource.BinarySI)

		// Check if this node group has enough GPU memory
		if totalGPUMemoryPerNode.Cmp(modelTotalGPUMemoryRequired) >= 0 {
			filtered = append(filtered, group)
		}
	}

	return filtered
}

// +kubebuilder:object:generate=false
type GPUAlreadyAssignedFilter struct{}

func (f *GPUAlreadyAssignedFilter) Filter(ctx context.Context, workspace *Workspace, nodeGroups []GPUNodeGroup) []GPUNodeGroup {
	// For now, this filter doesn't do anything special
	// In a full implementation, this would check if GPUs are already allocated to other workspaces
	// This requires examining running pods and their resource requests
	return nodeGroups
}

// Scheduler is used internally for validation and is not part of the CRD API
// +kubebuilder:object:generate=false
type Scheduler struct {
	filters    []Filter
	nodeGroups []GPUNodeGroup
}

func NewScheduler(nodeGroups []GPUNodeGroup) *Scheduler {
	return &Scheduler{
		filters: []Filter{
			&GPUCountFilter{},
			&GPUMemoryFilter{},
			&GPUAlreadyAssignedFilter{},
		},
		nodeGroups: nodeGroups,
	}
}

func (s *Scheduler) Schedule(ctx context.Context, workspace *Workspace) *GPUNodeGroup {
	filteredNodeGroups := s.nodeGroups

	// Apply all filters
	for _, filter := range s.filters {
		filteredNodeGroups = filter.Filter(ctx, workspace, filteredNodeGroups)
	}

	// If no node groups pass all filters, return nil
	if len(filteredNodeGroups) == 0 {
		return nil
	}

	// Find the node group with the most total memory (best resource utilization)
	var bestGroup *GPUNodeGroup
	var maxTotalMemory int64 = -1

	for i := range filteredNodeGroups {
		group := &filteredNodeGroups[i]
		totalMemory := group.GPUMemory.Value() * group.GPUCount

		if totalMemory > maxTotalMemory {
			maxTotalMemory = totalMemory
			bestGroup = group
		}
	}

	return bestGroup
}
