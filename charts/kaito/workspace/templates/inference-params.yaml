apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-params-template
  namespace: {{ .Release.Namespace }}
data:
  inference_config.yaml: |
    # Maximum number of steps to find the max available seq len fitting in the GPU memory.
    max_probe_steps: 6

    vllm:
      cpu-offload-gb: 0
      swap-space: 4

      # gpu-memory-utilization: 0.9
      # max-seq-len-to-capture: 8192
      # num-scheduler-steps: 1
      # enable-chunked-prefill: false
      # max-model-len: 2048
      # see https://docs.vllm.ai/en/stable/serving/engine_args.html for more options.