# Code Benchmark: System Architecture

## Overview

This document describes the architecture and design decisions of the Code Benchmark suite for comparing RAG and baseline LLM approaches in automated code modification.

## Complete Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          COMPLETE WORKFLOW                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PREREQUISITE: Index Your Code Repository
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python3 rag.py --repo . --url http://localhost:5000 \            â”‚
â”‚                 --index code_repo_benchmark                        â”‚
â”‚                                                                    â”‚
â”‚  Creates: Vector index of all code files (for RAG retrieval)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
STEP 1: Generate Test Issues from Indexed Code
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python3 generate_issues.py --repo . --output test_issues.txt     â”‚
â”‚                                                                    â”‚
â”‚  Input:   Scanned code repository structure                       â”‚
â”‚  Process: Analyze â†’ Identify components â†’ Generate realistic      â”‚
â”‚           issues based on actual code structure                   â”‚
â”‚  Output:  test_issues.txt (5 issues)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
STEP 2: Run Baseline Solution (Direct LLM with Manual Context)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python3 resolve_issues_baseline.py --issues test_issues.txt \    â”‚
â”‚                                      --output baseline_outputs/    â”‚
â”‚                                                                    â”‚
â”‚  Process:                                                          â”‚
â”‚    1. Read specified files (manual context)                       â”‚
â”‚    2. Call LLM with issue + context                               â”‚
â”‚    3. Parse JSON response (file modifications)                    â”‚
â”‚    4. Apply modifications to files                                â”‚
â”‚    5. Generate git diff                                           â”‚
â”‚    6. Run unit tests                                              â”‚
â”‚    7. Pass â†’ Keep changes                                         â”‚
â”‚       Fail â†’ Revert changes                                       â”‚
â”‚                                                                    â”‚
â”‚  Output:                                                           â”‚
â”‚    baseline_outputs/                                               â”‚
â”‚    â”œâ”€â”€ baseline_issue_001.diff       (git diff)                  â”‚
â”‚    â”œâ”€â”€ baseline_issue_001_tests.txt  (test results)              â”‚
â”‚    â”œâ”€â”€ baseline_issue_002.diff                                    â”‚
â”‚    â”œâ”€â”€ baseline_issue_002_tests.txt                               â”‚
â”‚    â””â”€â”€ baseline_summary_report.json  (success rate, tokens)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
STEP 3: Run RAG Solution (Automatic Retrieval with TOP-4 Filtering)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python3 rag_solution.py --issues test_issues.txt \               â”‚
â”‚                          --output rag_outputs/                     â”‚
â”‚                                                                    â”‚
â”‚  Process:                                                          â”‚
â”‚    1. Call RAG service with issue                                 â”‚
â”‚    2. RAG retrieves 100+ docs internally                          â”‚
â”‚    3. RAG returns 4-16 source_nodes with relevance scores         â”‚
â”‚    4. **TOP-4 FILTERING**: Sort by score, take top 4 files only   â”‚
â”‚    5. Parse RAG response (file modifications)                     â”‚
â”‚    6. Apply modifications to files                                â”‚
â”‚    7. Generate git diff                                           â”‚
â”‚    8. Run unit tests                                              â”‚
â”‚    9. Pass â†’ Keep changes                                         â”‚
â”‚       Fail â†’ Revert changes                                       â”‚
â”‚                                                                    â”‚
â”‚  Innovation: TOP-4 Filtering                                       â”‚
â”‚    â€¢ RAG returns 16 files: [0.5205, 0.4962, 0.4751, ...]         â”‚
â”‚    â€¢ Sort descending by relevance score                           â”‚
â”‚    â€¢ Take only TOP 4 â†’ 21.6% token savings                        â”‚
â”‚    â€¢ Improves context quality                                     â”‚
â”‚    â€¢ Log: "âœ“ TOP1: 0.5205 | file.go"                             â”‚
â”‚           "âœ— 0.4751 | other.go (filtered)"                        â”‚
â”‚                                                                    â”‚
â”‚  Output:                                                           â”‚
â”‚    rag_outputs/                                                    â”‚
â”‚    â”œâ”€â”€ rag_issue_001.diff            (git diff)                  â”‚
â”‚    â”œâ”€â”€ rag_issue_001_tests.txt       (test results)              â”‚
â”‚    â”œâ”€â”€ rag_issue_002.diff                                         â”‚
â”‚    â”œâ”€â”€ rag_issue_002_tests.txt                                    â”‚
â”‚    â””â”€â”€ rag_summary_report.json       (success rate, tokens)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
STEP 4: Compare Results & Generate Report
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python3 code_benchmark.py --baseline baseline_outputs/ \         â”‚
â”‚                             --rag rag_outputs/ \                   â”‚
â”‚                             --output comparison_report.json        â”‚
â”‚                                                                    â”‚
â”‚  Process:                                                          â”‚
â”‚    1. Load both summary reports (JSON)                            â”‚
â”‚    2. Calculate metrics:                                          â”‚
â”‚       â€¢ Success Rate: Pass/Total                                  â”‚
â”‚       â€¢ Token Efficiency: Avg tokens per issue                    â”‚
â”‚       â€¢ Files Modified: Number of changed files                   â”‚
â”‚       â€¢ Error Categories: Compilation errors, test failures       â”‚
â”‚    3. Compare baseline vs RAG                                     â”‚
â”‚    4. Determine winner                                            â”‚
â”‚    5. Generate recommendations                                    â”‚
â”‚                                                                    â”‚
â”‚  Output:                                                           â”‚
â”‚    comparison_report.json                                          â”‚
â”‚    {                                                               â”‚
â”‚      "baseline": {                                                 â”‚
â”‚        "success_rate": 0.20,                                       â”‚
â”‚        "avg_tokens": 12543,                                        â”‚
â”‚        "files_modified": 3                                         â”‚
â”‚      },                                                            â”‚
â”‚      "rag": {                                                      â”‚
â”‚        "success_rate": 0.60,                                       â”‚
â”‚        "avg_tokens": 9842,                                         â”‚
â”‚        "files_modified": 4                                         â”‚
â”‚      },                                                            â”‚
â”‚      "winner": "rag",                                              â”‚
â”‚      "token_savings": "21.6%",                                     â”‚
â”‚      "recommendations": [                                          â”‚
â”‚        "RAG provides better context coverage",                    â”‚
â”‚        "TOP-4 filtering balances quality and efficiency",        â”‚
â”‚        "Automatic retrieval outperforms manual selection"         â”‚
â”‚      ]                                                             â”‚
â”‚    }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## System Design

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Code Benchmark Suite                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Issue      â”‚    â”‚   Baseline   â”‚    â”‚     RAG      â”‚
â”‚  Generator    â”‚    â”‚   Solution   â”‚    â”‚   Solution   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â”‚                     â–¼                     â–¼
        â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚     LLM     â”‚      â”‚ RAG Service â”‚
        â”‚            â”‚   API       â”‚      â”‚   + LLM     â”‚
        â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Benchmark     â”‚
                    â”‚   Comparison     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Details

### 1. Issue Generator (`generate_issues.py`)

**Purpose**: Generate realistic test issues based on repository analysis

**Architecture**:

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CodebaseAnalyzer                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  scan_repository()                   â”‚ â”‚
â”‚  â”‚  - Walk directory tree               â”‚ â”‚
â”‚  â”‚  - Identify Go/Python files          â”‚ â”‚
â”‚  â”‚  - Build structure map               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  analyze_components()                â”‚ â”‚
â”‚  â”‚  - Extract packages/modules          â”‚ â”‚
â”‚  â”‚  - Identify controllers/services     â”‚ â”‚
â”‚  â”‚  - Map dependencies                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         IssueGenerator                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  generate_issues()                   â”‚ â”‚
â”‚  â”‚  - Use templates                     â”‚ â”‚
â”‚  â”‚  - Fill with component names         â”‚ â”‚
â”‚  â”‚  - Optional: LLM enhancement         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions**:

1. **Template-Based Generation**: Uses predefined templates to ensure issue quality
2. **Structure-Aware**: Analyzes actual codebase to generate relevant issues
3. **LLM Enhancement**: Optional LLM call for smarter, more realistic issues
4. **Language-Agnostic**: Supports multiple languages (Go, Python, etc.)

**Data Flow**:
```
Repository â†’ Scanner â†’ Components â†’ Templates â†’ Issues
                 â†“
            (Optional)
                LLM â†’ Enhanced Issues
```

### 2. Baseline Solution (`resolve_issues_baseline.py`)

**Purpose**: Resolve issues using direct LLM calls with manual context

**Architecture**:

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BaselineCodeModifier                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  read_relevant_files()                    â”‚ â”‚
â”‚  â”‚  - Identify files from issue context     â”‚ â”‚
â”‚  â”‚  - Read file contents                    â”‚ â”‚
â”‚  â”‚  - Limit to head_lines                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  call_llm()                               â”‚ â”‚
â”‚  â”‚  - System prompt (structure rules)       â”‚ â”‚
â”‚  â”‚  - User prompt (issue + context)         â”‚ â”‚
â”‚  â”‚  - Temperature = 0.0                     â”‚ â”‚
â”‚  â”‚  - Parse JSON response                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  apply_modifications()                    â”‚ â”‚
â”‚  â”‚  - Write modified files                  â”‚ â”‚
â”‚  â”‚  - Generate git diffs                    â”‚ â”‚
â”‚  â”‚  - Run tests                             â”‚ â”‚
â”‚  â”‚  - Revert if tests fail                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions**:

1. **Manual Context**: Developer provides file list, ensuring relevant context
2. **Temperature 0.0**: Deterministic output for reproducibility
3. **Head Lines Limiting**: Control token usage by limiting file lengths
4. **Test Validation**: Automatic compilation and test execution
5. **Auto-Revert**: Rolls back changes if tests fail

**Data Flow**:
```
Issue â†’ File Reader â†’ Context Builder â†’ LLM API
                                         â†“
                                    JSON Response
                                         â†“
        Test Results â† Test Runner â† File Writer
                                         â†“
                                    Git Diff
```

### 3. RAG Solution (`rag_solution.py`)

**Purpose**: Resolve issues using RAG service with automatic retrieval

**Architecture**:

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAGCodeModifier                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  call_rag()                                â”‚ â”‚
â”‚  â”‚  - Send issue to RAG API                  â”‚ â”‚
â”‚  â”‚  - RAG retrieves 100+ documents internallyâ”‚ â”‚
â”‚  â”‚  - Returns top-k source_nodes with scores â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â”‚                           â”‚
â”‚                      â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  _fix_file_paths_from_metadata()          â”‚ â”‚
â”‚  â”‚  - Extract source_nodes from response     â”‚ â”‚
â”‚  â”‚  - Read relevance scores                  â”‚ â”‚
â”‚  â”‚  - Sort by score (descending)             â”‚ â”‚
â”‚  â”‚  - Select TOP 4 files ONLY                â”‚ â”‚
â”‚  â”‚  - Filter out low-relevance files         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â”‚                           â”‚
â”‚                      â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  _parse_rag_response()                     â”‚ â”‚
â”‚  â”‚  - Parse JSON from RAG                    â”‚ â”‚
â”‚  â”‚  - Handle deepseek-specific format       â”‚ â”‚
â”‚  â”‚  - Extract file modifications            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â”‚                           â”‚
â”‚                      â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  apply_modifications()                     â”‚ â”‚
â”‚  â”‚  - Write files                            â”‚ â”‚
â”‚  â”‚  - Generate diffs                         â”‚ â”‚
â”‚  â”‚  - Run tests                              â”‚ â”‚
â”‚  â”‚  - Revert if failed                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions**:

1. **Automatic Retrieval**: No manual file selection needed
2. **TOP-4 Filtering**: Hard limit on context to prevent overload
3. **Relevance-Based**: Uses cosine similarity scores from RAG
4. **Enhanced System Prompt**: Strong warnings about structure preservation
5. **Source Node Validation**: Ensures metadata is available

**Critical Implementation Details**:

```python
# Relevance Filtering (Lines 385-445)
def _fix_file_paths_from_metadata(self, parsed_response, rag_result):
    MAX_FILES = 4  # Hard limit
    
    # Extract scores
    file_path_scores = {}
    for node in rag_result.get('source_nodes', []):
        score = node.get('score', 0.0)
        file_path = node['metadata']['file_path']
        file_path_scores[file_path] = score
    
    # Sort and filter
    sorted_files = sorted(file_path_scores.items(), 
                         key=lambda x: x[1], 
                         reverse=True)
    top_files = sorted_files[:MAX_FILES]
    
    # Log filtering
    print(f"  ğŸ“‹ Relevance scores for all {len(sorted_files)} files:")
    for i, (path, score) in enumerate(sorted_files, 1):
        if i <= MAX_FILES:
            print(f"     âœ“ TOP{i}: {score:.4f} | {path}")
        else:
            print(f"     âœ— {score:.4f} | {path}")
    
    return {path for path, score in top_files}
```

**RAG Service Integration**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAG Service (Port 5000)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  /v1/chat/completions              â”‚ â”‚
â”‚  â”‚  - Receives: messages, model, etc. â”‚ â”‚
â”‚  â”‚  - Returns: response + source_nodesâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                         â”‚
â”‚                â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Vector Store Query                â”‚ â”‚
â”‚  â”‚  - Calculate: top_k = max(100, ...) â”‚ â”‚
â”‚  â”‚  - Retrieve 100+ documents         â”‚ â”‚
â”‚  â”‚  - Rank by similarity              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                         â”‚
â”‚                â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LLM Context Building              â”‚ â”‚
â”‚  â”‚  - Include top documents           â”‚ â”‚
â”‚  â”‚  - Build prompt                    â”‚ â”‚
â”‚  â”‚  - Call LLM                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                         â”‚
â”‚                â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Response Assembly                 â”‚ â”‚
â”‚  â”‚  - LLM response                    â”‚ â”‚
â”‚  â”‚  - Source nodes with metadata      â”‚ â”‚
â”‚  â”‚  - Relevance scores                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow**:
```
Issue â†’ RAG API â†’ Internal Retrieval (100+ docs)
                        â†“
                 Rank by Similarity
                        â†“
                 Build LLM Context
                        â†“
                 LLM Generation
                        â†“
              Response + Source Nodes
                        â†“
       Python Client (TOP-4 Filter)
                        â†“
            Apply Modifications
```

### 4. Benchmark Comparison (`code_benchmark.py`)

**Purpose**: Compare results from baseline and RAG solutions

**Architecture**:

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BenchmarkComparator               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  load_reports()                  â”‚ â”‚
â”‚  â”‚  - Parse baseline JSON           â”‚ â”‚
â”‚  â”‚  - Parse RAG JSON                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                       â”‚
â”‚                â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  compare_success_rates()         â”‚ â”‚
â”‚  â”‚  - Pass vs Fail counts           â”‚ â”‚
â”‚  â”‚  - Percentage calculation        â”‚ â”‚
â”‚  â”‚  - Statistical significance      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                       â”‚
â”‚                â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  compare_token_usage()           â”‚ â”‚
â”‚  â”‚  - Total tokens                  â”‚ â”‚
â”‚  â”‚  - Average per issue             â”‚ â”‚
â”‚  â”‚  - Efficiency ratio              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                       â”‚
â”‚                â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  analyze_errors()                â”‚ â”‚
â”‚  â”‚  - Categorize failure types      â”‚ â”‚
â”‚  â”‚  - Common patterns               â”‚ â”‚
â”‚  â”‚  - Recommendations               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Design Patterns

### 1. Template Method Pattern

Used in both baseline and RAG solutions:

```python
class CodeModifier:
    def resolve_issue(self, issue):
        # Template method
        context = self.get_context(issue)      # Abstract
        response = self.call_ai(issue, context)  # Abstract
        self.apply_modifications(response)      # Concrete
        self.run_tests()                        # Concrete
        self.generate_report()                  # Concrete
```

### 2. Strategy Pattern

Different AI strategies (baseline vs RAG):

```python
class BaselineStrategy:
    def get_context(self, issue):
        return self.read_files_manually()

class RAGStrategy:
    def get_context(self, issue):
        return self.retrieve_from_index()
```

### 3. Observer Pattern

Progress tracking:

```python
class ProgressTracker:
    def notify(self, event, data):
        print(f"  {event}: {data}")

modifier.add_observer(ProgressTracker())
```

## Configuration Management

### Environment Variables

```bash
# LLM Configuration
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-...
LLM_MODEL=gpt-4

# RAG Configuration
RAG_SERVICE_URL=http://localhost:5000
RAG_INDEX_NAME=my_repo_index

# Benchmark Configuration
TEMPERATURE=0.0
MAX_TOKENS=40000
```

### Runtime Configuration

```python
# Baseline
baseline_config = {
    'head_lines': 500,
    'temperature': 0.0,
    'model': 'gpt-4'
}

# RAG
rag_config = {
    'max_files': 4,
    'temperature': 0.0,
    'context_token_ratio': 0.7
}
```

## Performance Considerations

### Token Optimization

**Baseline**:
- Limit file length with `head_lines`
- Selective file inclusion
- Efficient prompt structure

**RAG**:
- TOP-4 filtering (hard limit)
- Relevance score threshold
- Context/response ratio tuning

### Scalability

**Parallel Processing**:
```python
# Process multiple issues in parallel
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(resolve_issue, issue) 
               for issue in issues]
```

**Rate Limiting**:
```python
import time

def with_rate_limit(func):
    def wrapper(*args, **kwargs):
        time.sleep(1)  # 1 second delay
        return func(*args, **kwargs)
    return wrapper
```

## Error Handling

### Retry Strategy

```python
def call_with_retry(func, max_retries=3):
    for retry in range(max_retries):
        try:
            return func()
        except Exception as e:
            if retry < max_retries - 1:
                wait = 2 ** retry  # Exponential backoff
                print(f"  âš ï¸ Retrying in {wait}s...")
                time.sleep(wait)
            else:
                raise
```

### Graceful Degradation

```python
def resolve_issue_safe(issue):
    try:
        return resolve_issue(issue)
    except APIError:
        print("  âœ— API failed, saving raw response")
        save_raw_response()
        return None
    except TestError:
        print("  âœ— Tests failed, reverting changes")
        revert_changes()
        return None
```

## Testing Strategy

### Unit Tests

```python
def test_relevance_filtering():
    nodes = [
        {'score': 0.9, 'metadata': {'file_path': 'a.go'}},
        {'score': 0.8, 'metadata': {'file_path': 'b.go'}},
        {'score': 0.7, 'metadata': {'file_path': 'c.go'}},
        {'score': 0.6, 'metadata': {'file_path': 'd.go'}},
        {'score': 0.5, 'metadata': {'file_path': 'e.go'}},
    ]
    
    filtered = filter_top_k(nodes, k=4)
    assert len(filtered) == 4
    assert filtered[0]['file_path'] == 'a.go'
```

### Integration Tests

```python
def test_end_to_end():
    # Generate issues
    issues = generate_issues(repo='test_repo', count=2)
    
    # Run baseline
    baseline_results = resolve_baseline(issues)
    
    # Run RAG
    rag_results = resolve_rag(issues)
    
    # Compare
    comparison = compare(baseline_results, rag_results)
    
    assert comparison.success_rate > 0
```

## Future Enhancements

### Planned Features

1. **Multi-Model Support**: Test multiple LLMs in parallel
2. **Custom Metrics**: User-defined success criteria
3. **Confidence Scores**: RAG should return confidence for each change
4. **Interactive Mode**: Human-in-the-loop validation
5. **Continuous Benchmarking**: Automated daily runs

### Architectural Improvements

1. **Plugin System**: Easy addition of new AI strategies
2. **Database Backend**: Store results in SQLite/Postgres
3. **Web Dashboard**: Real-time progress monitoring
4. **API Layer**: RESTful API for remote execution

## Conclusion

The Code Benchmark architecture is designed for:

- **Modularity**: Easy to extend with new strategies
- **Reproducibility**: Deterministic results (temperature=0.0)
- **Observability**: Detailed logging and reporting
- **Scalability**: Parallel execution support
- **Robustness**: Comprehensive error handling

The key innovation is the **TOP-4 relevance filtering** in RAG solution, which balances context quality with token efficiency.
