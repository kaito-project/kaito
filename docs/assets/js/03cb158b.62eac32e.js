"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[8403],{28453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>c});var s=a(96540);const i={},t=s.createContext(i);function r(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},37865:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/keda-kaito-scaler-arch-1af338819a90073a9d0487574a964222.png"},95741:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"keda-autoscaler-inference","title":"KEDA Auto-Scaler for inference workloads","description":"- Feature status: Alpha","source":"@site/docs/keda-autoscaler-inference.md","sourceDirName":".","slug":"/keda-autoscaler-inference","permalink":"/kaito/docs/next/keda-autoscaler-inference","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/docs/keda-autoscaler-inference.md","tags":[],"version":"current","frontMatter":{"title":"KEDA Auto-Scaler for inference workloads"},"sidebar":"sidebar","previous":{"title":"Multi-Node Inference","permalink":"/kaito/docs/next/multi-node-inference"},"next":{"title":"Fine Tuning","permalink":"/kaito/docs/next/tuning"}}');var i=a(74848),t=a(28453);const r={title:"KEDA Auto-Scaler for inference workloads"},c=void 0,l={},o=[{value:"Introduction",id:"introduction",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Getting started",id:"getting-started",level:2},{value:"Install KEDA",id:"install-keda",level:3},{value:"Enable InferenceSet controller during KAITO install",id:"enable-inferenceset-controller-during-kaito-install",level:3},{value:"Example Scenarios",id:"example-scenarios",level:2},{value:"Time-Based KEDA Scaler",id:"time-based-keda-scaler",level:3},{value:"Example: Business Hours Scaling",id:"example-business-hours-scaling",level:4},{value:"Metric-Based KEDA Scaler",id:"metric-based-keda-scaler",level:3},{value:"Install KEDA KAITO Scaler",id:"install-keda-kaito-scaler",level:4},{value:"Example: Create a KAITO InferenceSet with annotations for running inference workloads",id:"example-create-a-kaito-inferenceset-with-annotations-for-running-inference-workloads",level:4},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Feature status: Alpha"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"LLM inference service is a basic and widely used feature in KAITO. As the number of waiting inference requests increases, you should scale more inference instances to prevent blocking. Conversely, reduce inference instances when requests decline to improve GPU resource utilization. Kubernetes Event Driven Autoscaling (KEDA) is well-suited for inference pod autoscaling. It enables event-driven, fine-grained scaling based on external metrics and triggers. KEDA supports a wide range of event sources (like custom metrics), allowing pods to scale precisely in response to workload demand. This flexibility and extensibility make KEDA ideal for dynamic, cloud-native applications that require responsive and efficient autoscaling."}),"\n",(0,i.jsx)(n.p,{children:"To enable intelligent autoscaling for KAITO inference workloads using service monitoring metrics, utilize the following components and features:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/kedacore/keda",children:"Kubernetes Event Driven Autoscaling (KEDA)"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://github.com/kaito-project/keda-kaito-scaler",children:"KEDA KAITO Scaler"})}),": A dedicated KEDA external scaler, eliminating the need for external dependencies such as Prometheus."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsxs)(n.strong,{children:["KAITO ",(0,i.jsx)(n.code,{children:"InferenceSet"})," CustomResourceDefinition (CRD) and controller"]}),": A new CRD and controller were built on top of the KAITO workspace for intelligent autoscaling, introduced as an alpha feature in KAITO version ",(0,i.jsx)(n.code,{children:"v0.8.0"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The following diagram shows how KEDA KAITO Scaler integrates KAITO InferenceSet with KEDA to autoscale inference workloads on AKS:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"keda-kaito-scaler-arch",src:a(37865).A+"",width:"2433",height:"1221"})}),"\n",(0,i.jsx)(n.h2,{id:"getting-started",children:"Getting started"}),"\n",(0,i.jsx)(n.h3,{id:"install-keda",children:"Install KEDA"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["The following example demonstrates how to install KEDA 2.x using Helm chart. For instructions on installing KEDA through other methods, refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/kedacore/keda#deploying-keda",children:"KEDA deployment documentation"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"export KEDA_NAMESPACE=kaito-workspace\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm install keda kedacore/keda --namespace $KEDA_NAMESPACE --create-namespace\n"})}),"\n",(0,i.jsx)(n.h3,{id:"enable-inferenceset-controller-during-kaito-install",children:"Enable InferenceSet controller during KAITO install"}),"\n",(0,i.jsx)(n.p,{children:"This feature is available starting from KAITO v0.8.0, and the InferenceSet Controller must be enabled during the KAITO installation."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export CLUSTER_NAME=kaito\n\nhelm repo add kaito https://kaito-project.github.io/kaito/charts/kaito\nhelm repo update\nhelm upgrade --install kaito-workspace kaito/workspace \\\n  --namespace kaito-workspace \\\n  --create-namespace \\\n  --set clusterName="$CLUSTER_NAME" \\\n  --set featureGates.enableInferenceSetController=true \\\n  --wait\n'})}),"\n",(0,i.jsx)(n.h2,{id:"example-scenarios",children:"Example Scenarios"}),"\n",(0,i.jsx)(n.h3,{id:"time-based-keda-scaler",children:"Time-Based KEDA Scaler"}),"\n",(0,i.jsxs)(n.p,{children:["The KEDA cron scaler enables scaling of workloads according to time-based schedules, making it especially beneficial for workloads with predictable traffic patterns. It is perfect for situations where peak hours are known ahead of time, allowing you to proactively adjust resources before demand rises. For more details about time-based scalers, refer to ",(0,i.jsx)(n.a,{href:"https://keda.sh/docs/2.18/scalers/cron/",children:"Scale applications based on a cron schedule"}),"."]}),"\n",(0,i.jsx)(n.h4,{id:"example-business-hours-scaling",children:"Example: Business Hours Scaling"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Create a KAITO InferenceSet for running inference workloads"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The following example creates an InferenceSet for the phi-4-mini model:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cat <<EOF | kubectl apply -f -\napiVersion: kaito.sh/v1alpha1\nkind: InferenceSet\nmetadata:\n  name: phi-4-mini\n  namespace: default\nspec:\n  labelSelector:\n    matchLabels:\n      apps: phi-4-mini\n  replicas: 1\n  template:\n    inference:\n      preset:\n        accessMode: public\n        name: phi-4-mini-instruct\n    resource:\n      instanceType: Standard_NC24ads_A100_v4\nEOF\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Create a KEDA ScaledObject"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Below is an example of creating a ",(0,i.jsx)(n.code,{children:"ScaledObject"})," that scales a KAITO InferenceSet based on business hours:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scale up to 5 replicas"})," from 6:00 AM to 8:00 PM (peak hours)"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scale down to 1 replica"})," otherwise (off-peak hours)"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'cat <<EOF | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kaito-business-hours-scaler\n  namespace: default\nspec:\n  # Target KAITO InferenceSet to scale\n  scaleTargetRef:\n    apiVersion: kaito.sh/v1alpha1\n    kind: InferenceSet\n    name: phi-4-mini\n  # Scaling boundaries\n  minReplicaCount: 1\n  maxReplicaCount: 5\n  # Cron-based triggers for time-based scaling\n  triggers:\n  # Scale up to 5 replicas at 6:00 AM (start of business hours)\n  - type: cron\n    metadata:\n      timezone: "America/New_York"  # Adjust timezone as needed\n      start: "0 6 * * 1-5"          # 6:00 AM Monday to Friday\n      end: "0 20 * * 1-5"           # 8:00 PM Monday to Friday\n      desiredReplicas: "5"          # Scale to 5 replicas during business hours\n  # Scale down to 1 replica at 8:00 PM (end of business hours)\n  - type: cron\n    metadata:\n      timezone: "America/New_York"  # Adjust timezone as needed\n      start: "0 20 * * 1-5"         # 8:00 PM Monday to Friday\n      end: "0 6 * * 1-5"            # 6:00 AM Monday to Friday (next day)\n      desiredReplicas: "1"          # Scale to 1 replica during off-hours\nEOF\n'})}),"\n",(0,i.jsx)(n.h3,{id:"metric-based-keda-scaler",children:"Metric-Based KEDA Scaler"}),"\n",(0,i.jsx)(n.h4,{id:"install-keda-kaito-scaler",children:"Install KEDA KAITO Scaler"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"This component is required only when using metric-based KEDA scaler. Ensure that KEDA KAITO Scaler is installed within the same namespace as KEDA."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"helm repo add keda-kaito-scaler https://kaito-project.github.io/keda-kaito-scaler/charts/kaito-project\nhelm upgrade --install keda-kaito-scaler -n $KEDA_NAMESPACE keda-kaito-scaler/keda-kaito-scaler --create-namespace\n"})}),"\n",(0,i.jsxs)(n.p,{children:["After a few seconds, the ",(0,i.jsx)(n.code,{children:"keda-kaito-scaler"})," deployment starts."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# kubectl get deployment keda-kaito-scaler -n $KEDA_NAMESPACE\nNAME                READY   UP-TO-DATE   AVAILABLE   AGE\nkeda-kaito-scaler   1/1     1            1           28h\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"keda-kaito-scaler"})," provides a simplified configuration interface for scaling vLLM inference workloads. It directly scrapes metrics from inference pods, eliminating the need for a separate monitoring stack."]}),"\n",(0,i.jsx)(n.h4,{id:"example-create-a-kaito-inferenceset-with-annotations-for-running-inference-workloads",children:"Example: Create a KAITO InferenceSet with annotations for running inference workloads"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The following example creates an InferenceSet for the phi-4-mini model, using annotations with the prefix ",(0,i.jsx)(n.code,{children:"scaledobject.kaito.sh/"})," to supply parameter inputs for the KEDA KAITO scaler."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"scaledobject.kaito.sh/auto-provision"})," (required): When set to ",(0,i.jsx)(n.code,{children:"true"}),", the KEDA KAITO scaler automatically provisions a ScaledObject based on the ",(0,i.jsx)(n.code,{children:"InferenceSet"})," object."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"scaledobject.kaito.sh/max-replicas"})," (required): Maximum number of replicas for the target InferenceSet."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"scaledobject.kaito.sh/metricName"})," (optional): Specifies the metric name collected from the vLLM pod, which is used for monitoring and triggering the scaling operation. The default is ",(0,i.jsx)(n.code,{children:"vllm:num_requests_waiting"}),". For all vLLM metrics, see ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/usage/metrics/#general-metrics",children:"vLLM Production Metrics"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"scaledobject.kaito.sh/threshold"})," (required): Specifies the threshold for the monitored metric that triggers the scaling operation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'cat <<EOF | kubectl apply -f -\napiVersion: kaito.sh/v1alpha1\nkind: InferenceSet\nmetadata:\n  annotations:\n    scaledobject.kaito.sh/auto-provision: "true"\n    scaledobject.kaito.sh/max-replicas: "5"\n    scaledobject.kaito.sh/metricName: "vllm:num_requests_waiting"\n    scaledobject.kaito.sh/threshold: "10"\n  name: phi-4-mini\n  namespace: default\nspec:\n  labelSelector:\n    matchLabels:\n      apps: phi-4-mini\n  replicas: 1\n  template:\n    inference:\n      preset:\n        accessMode: public\n        name: phi-4-mini-instruct\n    resource:\n      instanceType: Standard_NC24ads_A100_v4\nEOF\n'})}),"\n",(0,i.jsxs)(n.p,{children:["In just a few seconds, the KEDA KAITO scaler automatically creates the ",(0,i.jsx)(n.code,{children:"scaledobject"})," and ",(0,i.jsx)(n.code,{children:"hpa"})," objects. After a few minutes, once the inference pod runs, the KEDA KAITO scaler begins scraping ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/usage/metrics/#general-metrics",children:"metric values"})," from the inference pod. The system then marks the status of the ",(0,i.jsx)(n.code,{children:"scaledobject"})," and ",(0,i.jsx)(n.code,{children:"hpa"})," objects as ready."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# kubectl get scaledobject\nNAME           SCALETARGETKIND                  SCALETARGETNAME   MIN   MAX   READY   ACTIVE    FALLBACK   PAUSED   TRIGGERS   AUTHENTICATIONS           AGE\nphi-4-mini     kaito.sh/v1alpha1.InferenceSet   phi-4-mini        1     5     True    True     False      False    external   keda-kaito-scaler-creds   10m\n\n# kubectl get hpa\nNAME                    REFERENCE                   TARGETS      MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-phi-4-mini     InferenceSet/phi-4-mini     0/10 (avg)   1         5         1          11m\n"})}),"\n",(0,i.jsxs)(n.p,{children:["That's it! Your KAITO workloads will now automatically scale based on the average number of waiting inference requests(",(0,i.jsx)(n.code,{children:"vllm:num_requests_waiting"}),") across all workloads associated with ",(0,i.jsx)(n.code,{children:"InferenceSet/phi-4-mini"})," in the cluster."]}),"\n",(0,i.jsxs)(n.p,{children:["In the example below, if ",(0,i.jsx)(n.code,{children:"vllm:num_requests_waiting"})," exceeds the threshold (10) for over 60 seconds, KEDA scales up by adding a new replica to ",(0,i.jsx)(n.code,{children:"InferenceSet/phi-4-mini"}),". Conversely, if ",(0,i.jsx)(n.code,{children:"vllm:num_requests_waiting"})," remains below the threshold (10) for more than 300 seconds, KEDA scales down the number of replicas."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'Every 2.0s: kubectl describe hpa\nName:                                                     keda-hpa-phi-4-mini\nNamespace:                                                default\nLabels:                                                   app.kubernetes.io/managed-by=keda-operator\n                                                          app.kubernetes.io/name=keda-hpa-phi-4-mini\n                                                          app.kubernetes.io/part-of=phi-4-mini\n                                                          app.kubernetes.io/version=2.18.1\n                                                          scaledobject.keda.sh/name=phi-4-mini\nAnnotations:                                              scaledobject.kaito.sh/managed-by: keda-kaito-scaler\nCreationTimestamp:                                        Tue, 09 Dec 2025 03:35:09 +0000\nReference:                                                InferenceSet/phi-4-mini\nMetrics:                                                  ( current / target )\n  "s0-vllm:num_requests_waiting" (target average value):  58 / 10\nMin replicas:                                             1\nMax replicas:                                             5\nBehavior:\n  Scale Up:\n    Stabilization Window: 60 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Pods  Value: 1  Period: 300 seconds\n  Scale Down:\n    Stabilization Window: 300 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Pods  Value: 1  Period: 600 seconds\nInferenceSet pods:  2 current / 2 desired\nConditions:\n  Type            Status  Reason            Message\n  ----            ------  ------            -------\n  AbleToScale     True    ReadyForNewScale  recommended size matches current size\n  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from external metric s0-vllm:num_requests_waiting(&Lab\nelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: phi-4-mini,},MatchExpressions:[]LabelSelectorRequirement{},})\n  ScalingLimited  True    ScaleUpLimit      the desired replica count is increasing faster than the maximum scale rate\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  SuccessfulRescale  33s   horizontal-pod-autoscaler  New size: 2; reason: external metric s0-vllm:num_requests_waiting(&LabelSelector{MatchLabels:ma\np[string]string{scaledobject.keda.sh/name: phi-4-mini,},MatchExpressions:[]LabelSelectorRequirement{},}) above target\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"KAITO's LLM inference service must scale inference instances dynamically to handle varying numbers of waiting requests: scaling up to prevent blocking when requests increase, and scaling down to optimize GPU usage when requests decrease. With the newly introduced InferenceSet CRD and KEDA KAITO scaler, configuring this setting in KAITO has become much simpler."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);