deployment:
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: WORKLOAD_NAME
  spec:
    progressDeadlineSeconds: 1800
    replicas: 1
    selector:
      matchLabels:
        app: WORKLOAD_NAME
    template:
      metadata:
        labels:
          app: WORKLOAD_NAME
      spec:
        containers:
        - name: WORKLOAD_NAME-container
          image: REPO/MODEL_NAME:TAG
          command:
            - /bin/sh
            - -c
            - RUNTIME_COMMAND
          resources:
            requests:
              nvidia.com/gpu: GPU_COUNT
            limits:
              nvidia.com/gpu: GPU_COUNT
          livenessProbe:
            httpGet:
              path: /health
              port: 5000
            initialDelaySeconds: 600 # 10 Min
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 5000
            initialDelaySeconds: 30
            periodSeconds: 10
          volumeMounts:
          - name: dshm
            mountPath: /dev/shm
          - name: config-volume
            mountPath: /mnt/config
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: config-volume
          configMap:
            defaultMode: 420
            name: testing-inference-params
        tolerations:
        - effect: NoSchedule
          key: sku
          operator: Equal
          value: gpu
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
        nodeSelector:
          pool: NODE_POOL

config:
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: testing-inference-params
  data:
    inference_config.yaml: |
      # Maximum number of steps to find the max available seq len fitting in the GPU memory.
      max_probe_steps: 6

      vllm:
        cpu-offload-gb: 0
        gpu-memory-utilization: 0.95
        swap-space: 4
        served-model-name: test
        dtype: float16

        # max-seq-len-to-capture: 8192
        # num-scheduler-steps: 1
        # enable-chunked-prefill: false
        # see https://docs.vllm.ai/en/stable/models/engine_args.html for more options.

service:
  apiVersion: v1
  kind: Service
  metadata:
    name: WORKLOAD_NAME
  spec:
    ports:
    - port: 5000
      targetPort: 5000
      protocol: TCP
      name: http
    selector:
      app: WORKLOAD_NAME
    type: ClusterIP